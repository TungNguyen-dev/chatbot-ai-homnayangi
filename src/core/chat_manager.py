"""
High-level chat manager that orchestrates all components.
"""

from typing import Generator, Union, cast

from openai.types.chat import ChatCompletionMessageParam

from src.context.embeddings import EmbeddingsManager
from src.context.memory_manager import MemoryManager
from src.core.llm_client import LLMClient
from src.core.prompt_builder import PromptBuilder


class ChatManager:
    """Manages chat interactions and coordinates all components."""

    def __init__(self):
        self.memory = MemoryManager()
        self.embeddings = EmbeddingsManager()
        self.llm = LLMClient()
        self.prompt_builder = PromptBuilder()

    def send_message(self, user_message: str, stream: bool = False) -> Union[
        str, Generator[str, None, None]]:
        """
        Send a user message and get a response.

        Args:
            user_message: The user's message
            stream: Whether to stream the response

        Returns:
            The assistant's response (str) or a streaming generator of response chunks
        """
        # Add the user message to memory
        self.memory.add_message("user", user_message)

        # Optionally add to vector DB for long-term memory
        if self.embeddings.enabled:
            self.embeddings.add_text(user_message, metadata={"role": "user"})

        # ðŸ†• 3ï¸âƒ£ Truy váº¥n vector DB xem cÃ³ mÃ³n nÃ o phÃ¹ há»£p vá»›i cÃ¢u há»i hoáº·c sá»Ÿ thÃ­ch khÃ´ng
        similar_items = []
        if self.embeddings.enabled:
            similar_items = self.embeddings.search_similar(user_message, n_results=3)

        # ðŸ†• 4ï¸âƒ£ Náº¿u cÃ³ káº¿t quáº£, táº¡o Ä‘oáº¡n context Ä‘á»ƒ AI dÃ¹ng
        context_info = ""
        if similar_items:
            context_info = (
                "DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i mÃ³n Äƒn báº¡n cÃ³ thá»ƒ cÃ¢n nháº¯c (Æ°u tiÃªn theo sá»Ÿ thÃ­ch cá»§a ngÆ°á»i dÃ¹ng):\n"
                + "\n".join(f"- {item}" for item in similar_items)
            )
        print(context_info)

        # Build messages for LLM
        messages = self.prompt_builder.build_messages(self.memory.get_messages())

        # ThÃªm 1 tin nháº¯n â€œsystemâ€ má»›i chá»©a gá»£i Ã½ mÃ³n Äƒn gáº§n nháº¥t
        if context_info:
            messages.insert(1, {"role": "system", "content": context_info})

        # âœ… Explicitly cast to maintain type safety
        typed_messages = cast(list[ChatCompletionMessageParam], cast(object, messages))

        # Generate response
        if stream:
            return self._generate_streaming_response(typed_messages)
        else:
            response = self.llm.generate_response(typed_messages)
            self.memory.add_message("assistant", response)
            return response

    def _generate_streaming_response(
            self, messages: list[ChatCompletionMessageParam]
    ) -> Generator[str, None, None]:
        """
        Generate a streaming response.
        """
        full_response = ""

        for chunk in self.llm.generate_response_stream(messages):
            if chunk:
                yield chunk
                full_response += chunk

        # Add a complete response to memory after streaming
        self.memory.add_message("assistant", full_response)

    def get_conversation_history(self):
        """Get the current conversation history."""
        return self.memory.get_messages()

    def clear_conversation(self):
        """Clear the conversation history."""
        self.memory.clear()

    def get_context_summary(self) -> str:
        """Get a summary of the current context."""
        return self.memory.get_context_summary()
