"""
High-level chat manager that orchestrates all components.
"""

from typing import Generator, Union, cast

from openai.types.chat import ChatCompletionMessageParam

from src.context.embeddings import EmbeddingsManager
from src.context.memory_manager import MemoryManager
from src.core.llm_client import LLMClient
from src.core.prompt_builder import PromptBuilder


class ChatManager:
    """Manages chat interactions and coordinates all components."""

    def __init__(self):
        self.memory = MemoryManager()
        self.embeddings = EmbeddingsManager()
        self.llm = LLMClient()
        self.prompt_builder = PromptBuilder()

    def send_message(self, user_message: str, stream: bool = False) -> Union[
        str, Generator[str, None, None]]:
        """
        Send a user message and get a response.

        Args:
            user_message: The user's message
            stream: Whether to stream the response

        Returns:
            The assistant's response (str) or a streaming generator of response chunks
        """
        # Add the user message to memory
        self.memory.add_message("user", user_message)

        # Optionally add to vector DB for long-term memory
        if self.embeddings.enabled:
            self.embeddings.add_text(user_message, metadata={"role": "user"})

        # ðŸ†• 3ï¸âƒ£ Truy váº¥n vector DB xem cÃ³ mÃ³n nÃ o phÃ¹ há»£p vá»›i cÃ¢u há»i hoáº·c sá»Ÿ thÃ­ch khÃ´ng
        similar_items = []
        if self.embeddings.enabled:
            similar_items = self.embeddings.search_similar(user_message, n_results=3)

        # ðŸ†• 4ï¸âƒ£ Náº¿u cÃ³ káº¿t quáº£, táº¡o Ä‘oáº¡n context Ä‘á»ƒ AI dÃ¹ng
        context_info = ""
        if similar_items:
            context_info = (
                    "ThÃ´ng tin tham kháº£o Ä‘Æ°á»£c truy xuáº¥t tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u (cÃ³ thá»ƒ há»¯u Ã­ch cho cÃ¢u há»i):\n\n"
                    + "\n".join(f"- {item}" for item in similar_items)
            )

        # Build messages for LLM
        messages = self.prompt_builder.build_messages(self.memory.get_messages())

        # 5ï¸âƒ£ ChÃ¨n system message chá»©a context (Æ°u tiÃªn ngay sau system Ä‘áº§u tiÃªn)
        if context_info:
            # Táº¡o prompt rÃµ rÃ ng cho LLM biáº¿t cÃ¡ch dÃ¹ng context
            rag_prompt = {
                "role": "system",
                "content": (
                    "Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn tÆ° váº¥n vá» áº©m thá»±c. "
                    "HÃ£y sá»­ dá»¥ng thÃ´ng tin dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ giÃºp tráº£ lá»i cÃ¢u há»i ngÆ°á»i dÃ¹ng náº¿u phÃ¹ há»£p.\n\n"
                    f"{context_info}"
                ),
            }

            # ChÃ¨n vÃ o sau message system Ä‘áº§u tiÃªn
            if messages and messages[0]["role"] == "system":
                messages.insert(1, rag_prompt)
            else:
                messages.insert(0, rag_prompt)

        # âœ… Explicitly cast to maintain type safety
        typed_messages = cast(list[ChatCompletionMessageParam], cast(object, messages))

        # Generate response
        if stream:
            return self._generate_streaming_response(typed_messages)
        else:
            response = self.llm.generate_response(typed_messages)
            self.memory.add_message("assistant", response)
            return response

    def _generate_streaming_response(
            self, messages: list[ChatCompletionMessageParam]
    ) -> Generator[str, None, None]:
        """
        Generate a streaming response.
        """
        full_response = ""

        for chunk in self.llm.generate_response_stream(messages):
            if chunk:
                yield chunk
                full_response += chunk

        # Add a complete response to memory after streaming
        self.memory.add_message("assistant", full_response)

    def get_conversation_history(self):
        """Get the current conversation history."""
        return self.memory.get_messages()

    def clear_conversation(self):
        """Clear the conversation history."""
        self.memory.clear()

    def get_context_summary(self) -> str:
        """Get a summary of the current context."""
        return self.memory.get_context_summary()
