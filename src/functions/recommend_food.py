import streamlit as st
from openai import OpenAI

from src.config.settings import settings
from src.context.embeddings import EmbeddingsManager
from src.core.prompt_builder import PromptBuilder
from src.utils.detect_ingredients import detect_ingredients
from src.utils.detect_user_type import detect_user_type
from src.utils.get_meal_time import get_meal_time_from_hour

DEFINITION = {
    "type": "function",
    "function": {
        "name": "recommend_food",
        "description": (
            "G·ª£i √Ω m√≥n ƒÉn ph√π h·ª£p d·ª±a tr√™n b·ªëi c·∫£nh h·ªôi tho·∫°i c·ªßa ng∆∞·ªùi d√πng, "
            "bao g·ªìm lo·∫°i ng∆∞·ªùi d√πng (c√° nh√¢n ho·∫∑c gia ƒë√¨nh), nguy√™n li·ªáu s·∫µn c√≥, "
            "v√† th·ªùi ƒëi·ªÉm trong ng√†y. "
            "H√†m n√†y ƒë∆∞·ª£c g·ªçi khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ vi·ªác n√™n ƒÉn g√¨, "
            "v√≠ d·ª•: 'H√¥m nay ƒÉn g√¨', 'T·ªëi nay ƒÉn g√¨', ho·∫∑c khi h·ªôi tho·∫°i c√≥ c√¢u h·ªèi t∆∞∆°ng t·ª±."
        ),
        "examples": [
            {
                "user_input": "T·ªëi nay c·∫£ nh√† ƒÉn g√¨ ƒë∆∞·ª£c?",
                "detected_context": {
                    "serving_type": "family",
                    "ingredients": "th·ªãt b√≤, rau c·∫£i",
                    "meal_type": "evening"
                },
                "example_call": {
                    "name": "recommend_food",
                    "arguments": {
                        "serving_type": "family",
                        "ingredients": "th·ªãt b√≤, rau c·∫£i",
                        "meal_type": "evening"
                    },
                },
                "example_output": {
                    "recommendation": "L·∫©u b√≤ ho·∫∑c b√≤ x√†o rau c·∫£i l√† l·ª±a ch·ªçn ngon mi·ªáng v√† ph√π h·ª£p cho b·ªØa t·ªëi gia ƒë√¨nh."
                },
            },
        ],
    },
}


def handle(llm_client, args: dict):
    """
    Handle user's food recommendation request.

    This function builds a contextual prompt to recommend suitable foods
    based on user's messages, serving type, available ingredients, and meal time.
    If the context is insufficient (e.g., unknown serving type), the function asks
    for clarification instead of calling the model.
    """
    print(st.session_state.messages)
    try:
        # === 1. Detect serving type ===
        serving_type = detect_user_type(llm_client, {"message": st.session_state.messages})
        if serving_type == "unknown":
            return "B·∫°n mu·ªën chu·∫©n b·ªã b·ªØa ƒÉn cho c√° nh√¢n hay cho gia ƒë√¨nh?"

        # === 2. Build dynamic prompt ===
        prompt_parts = ["Recommend foods based on user's preferences",
                        f"- Serving type: {serving_type}"]

        # Detect ingredients (if mentioned by the user)
        ingredients = detect_ingredients(st.session_state.messages)
        if ingredients:
            prompt_parts.append(f"- Ingredients: {ingredients}")

        # Determine the meal type from the current time
        meal_type = get_meal_time_from_hour()
        if meal_type:
            prompt_parts.append(f"- Meal type: {meal_type}")

        # Predefined prompt
        prompt_builder = PromptBuilder()
        sys_prompt = prompt_builder.build_system_message()
        prompt_parts.append(f"- Predefined prompt: {sys_prompt}")

        # History chat
        prompt_parts.append(f"- History chat: {st.session_state.messages}")

        # Combine all parts into a final system prompt
        messageList = []
        system_prompt = "\n".join(prompt_parts)
        messageList.append({"role": "system", "content": system_prompt})

        # Embeddings & RAG
        embeddings = EmbeddingsManager()
        user_message = st.session_state.messages[-1]['content']
        print("User message: " + user_message)
        embeddings.add_text(user_message, metadata={"role": "user"})

        # üÜï 3Ô∏è‚É£ Truy v·∫•n vector DB xem c√≥ m√≥n n√†o ph√π h·ª£p v·ªõi c√¢u h·ªèi ho·∫∑c s·ªü th√≠ch kh√¥ng
        similar_items = []
        if embeddings.enabled:
            similar_items = embeddings.search_similar(user_message, n_results=3)

        # üÜï 4Ô∏è‚É£ N·∫øu c√≥ k·∫øt qu·∫£, t·∫°o ƒëo·∫°n context ƒë·ªÉ AI d√πng
        context_info = ""
        if similar_items:
            context_info = (
                    "Th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c truy xu·∫•t t·ª´ c∆° s·ªü d·ªØ li·ªáu (c√≥ th·ªÉ h·ªØu √≠ch cho c√¢u h·ªèi):\n\n"
                    + "\n".join(f"- {item}" for item in similar_items)
            )

        # 5Ô∏è‚É£ Ch√®n system message ch·ª©a context (∆∞u ti√™n ngay sau system ƒë·∫ßu ti√™n)
        if context_info:
            # T·∫°o prompt r√µ r√†ng cho LLM bi·∫øt c√°ch d√πng context
            rag_prompt = {
                "role": "system",
                "content": (
                    "B·∫°n l√† tr·ª£ l√Ω AI chuy√™n t∆∞ v·∫•n v·ªÅ ·∫©m th·ª±c. "
                    "H√£y s·ª≠ d·ª•ng th√¥ng tin d∆∞·ªõi ƒë√¢y ƒë·ªÉ gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi ng∆∞·ªùi d√πng n·∫øu ph√π h·ª£p.\n\n"
                    f"{context_info}"
                ),
            }
            messageList.append(rag_prompt)

        # === 3. Initialize LLM client ===
        client = OpenAI(
            base_url=settings.OPENAI_BASE_URL,
            api_key=settings.OPENAI_API_KEY,
        )
        print(messageList)

        # === 4. Stream model response ===
        stream = client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=messageList,
            temperature=settings.OPENAI_TEMPERATURE,
            max_completion_tokens=settings.OPENAI_MAX_TOKENS,
            stream=True,
        )

        return st.write_stream(stream)

    except Exception as e:
        # Handle unexpected runtime issues gracefully
        print(f"[ERROR] Food recommendation failed: {e}")
        return "Xin l·ªói, h·ªá th·ªëng g·∫∑p l·ªói khi g·ª£i √Ω m√≥n ƒÉn. Vui l√≤ng th·ª≠ l·∫°i sau."
